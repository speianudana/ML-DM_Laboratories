{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.3. Use the dataset Data1.csv to implement the list of classification models below in order to predict whether a tumor is malign or beginning. The dependent variable in this dataset is Class, it is equal to 2 if the tumor is benign and 4 if it is malign. The remaining columns are features that describe different characteristics of each tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data1.csv')\n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now will split the dataset into the training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Logistic Regression library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Logistic Regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model on the data, storing the information learned from the data\n",
    "Model is learning the relationship between digits (x_train) and labels (y_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels for new data. Uses the information the model learned during the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.predict(x_test[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict for Multiple Observations at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 4, 2, 2, 2, 4, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on entire test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuracy of the model using the score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.62 %\n"
     ]
    }
   ],
   "source": [
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(\"Accuracy: {:.2f} %\".format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84  3]\n",
      " [ 3 47]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results above the accuracy of the model is not quite good - 95.62%. There were 6 values out of 137 that were not predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-fold cross-validation method (cv = 10) will ensure that the accuracy score calculated only on one test set was not just a lucky occurence. To be more exactly in k-fold, with K=10, the dataset is devided into 10 subsets. At the first iteration the model is trained on the first 9 subsets and tested on the 10th subset. In the second iteration, the model is trained on the first 8 subsets and 10th, and is tested on the 9th subset. In this way there are 10 iterations and we obtain 10 accuracies of the model. Will print the mean accuracy score and the standard deviation  of the score computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.70 %\n",
      "Standard Deviation: 1.97 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94545455, 0.96363636, 0.96363636, 1.        , 0.94545455,\n",
       "       1.        , 0.96296296, 0.96296296, 0.98148148, 0.94444444])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "logisticRegr_accuracies = cross_val_score(estimator = logisticRegr, X = x_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(logisticRegr_accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(logisticRegr_accuracies.std()*100))\n",
    "logisticRegr_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accurracy for 10 test sets is 96.7% that is not much bigger that the accuracy of the model calculated on only one train data set (95.62%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Decision Tree Model using Scikit-learn. Import Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Decision Tree classifer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier( random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Decision Tree Classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = dtc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the response for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate, how accurately the classifier or model can predict whether a tumor is malign or bening. Accuracy can be computed by comparing actual test set values and predicted values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.70 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f} %\".format(metrics.accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80  7]\n",
      " [ 3 47]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this model has a big accuracy that means it is good to use for predicting whether a tumor is malign or benign. The confusion matrix also shows that only 10 out of 137 objects are not predicted as they had to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-fold cross-validation method (cv = 10) will ensure that the accuracy score calculated only on one test set was not just a lucky occurence. To be more exactly in k-fold, with K=10, the dataset is devided into 10 subsets. At the first iteration the model is trained on the first 9 subsets and tested on the 10th subset. In the second iteration, the model is trained on the first 8 subsets and 10th, and is tested on the 9th subset. In this way there are 10 iterations and we obtain 10 accuracies of the model. Will print the mean accuracy score and the standard deviation  of the score computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.59 %\n",
      "Standard Deviation: 2.21 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92727273, 0.90909091, 0.92727273, 0.96363636, 0.94545455,\n",
       "       0.92727273, 0.90740741, 0.92592593, 0.98148148, 0.94444444])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "dtc_accuracies = cross_val_score(estimator = dtc, X = x_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(dtc_accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(dtc_accuracies.std()*100))\n",
    "dtc_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy calculated for 10 subsets of data is bigger with 0.89% that the accuracy of the model calculated on only one test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest Classifier (with nb_trees = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Random Forest Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Gaussian Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(n_estimators=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the training sets y_pred=clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the response for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, check the accuracy using actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.62 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f} %\".format(metrics.accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84  3]\n",
      " [ 3 47]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the accuracy of the model is pretty high - 96,35%. There are only 5 values that were not predicted to the correct group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-fold cross-validation method (cv = 10) will ensure that the accuracy score calculated only on one test set was not just a lucky occurence. To be more exactly in k-fold, with K=10, the dataset is devided into 10 subsets. At the first iteration the model is trained on the first 9 subsets and tested on the 10th subset. In the second iteration, the model is trained on the first 8 subsets and 10th, and is tested on the 9th subset. In this way there are 10 iterations and we obtain 10 accuracies of the model. Will print the mean accuracy score and the standard deviation  of the score computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.15 %\n",
      "Standard Deviation: 2.39 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92727273, 0.96363636, 0.98181818, 0.98181818, 0.94545455,\n",
       "       1.        , 0.92592593, 0.96296296, 0.98148148, 0.94444444])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rfc_accuracies = cross_val_score(estimator = rfc, X = x_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(rfc_accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(rfc_accuracies.std()*100))\n",
    "rfc_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe that the mean value of the accuracy for the model calculated on 10 test datasets (96.15%) is bigger than the value of the accuracy on the model calculated on only one test dataset (95.62%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K- Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the KNeighborsClassifier library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the object of KNeighborsClassifier type with number of neighbors =3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the KNeighbors Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the response for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neigh.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, check the accuracy using actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.08 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f} %\".format(metrics.accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85  2]\n",
      " [ 2 48]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that this model is better that the previous ones. This has 97.08% of accuracy and only 4 values out of 137 are predicted wrongly. For sure is one of the best candidates for the best models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-fold cross-validation method (cv = 10) will ensure that the accuracy score calculated only on one test set was not just a lucky occurence. To be more exactly in k-fold, with K=10, the dataset is devided into 10 subsets. At the first iteration the model is trained on the first 9 subsets and tested on the 10th subset. In the second iteration, the model is trained on the first 8 subsets and 10th, and is tested on the 9th subset. In this way there are 10 iterations and we obtain 10 accuracies of the model. Will print the mean accuracy score and the standard deviation  of the score computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.34 %\n",
      "Standard Deviation: 2.44 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.90909091, 0.94545455, 0.98181818, 0.98181818, 0.94545455,\n",
       "       1.        , 0.96296296, 0.96296296, 0.98148148, 0.96296296])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "neigh_accuracies = cross_val_score(estimator = neigh, X = x_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(neigh_accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(neigh_accuracies.std()*100))\n",
    "neigh_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the accuracy for the model calculated on only one test dataset is bigger than the mean accuracy for the model calculated on 10 smaller test data sets with 0.74%. Here we can say that there was a little lucky occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the GaussianNB library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the GaussianNB object and train it on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions on the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing actual response values (y_test) with predicted response values (y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy: {:.2f}%\".format(metrics.accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80  7]\n",
      " [ 0 50]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a pretty good accuracy- 94.89%. There are only 7 values that were predicted wrongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-fold cross-validation method (cv = 10) will ensure that the accuracy score calculated only on one test set was not just a lucky occurence. To be more exactly in k-fold, with K=10, the dataset is devided into 10 subsets. At the first iteration the model is trained on the first 9 subsets and tested on the 10th subset. In the second iteration, the model is trained on the first 8 subsets and 10th, and is tested on the 9th subset. In this way there are 10 iterations and we obtain 10 accuracies of the model. Will print the mean accuracy score and the standard deviation  of the score computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.16 %\n",
      "Standard Deviation: 1.91 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92727273, 0.96363636, 0.96363636, 0.96363636, 0.96363636,\n",
       "       0.96363636, 0.98148148, 0.94444444, 1.        , 0.94444444])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "gnb_accuracies = cross_val_score(estimator = gnb, X = x_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(gnb_accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(gnb_accuracies.std()*100))\n",
    "gnb_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be observed that the accuracy score of the model calculated on only one test set is smaller that the accuracy mean calculated on 10 smaller data sets. (94.89% < 96.16%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(random_state=0)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions and compare the results by displaying the predicted values of y next to the test values of y in a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 4, 2, 2, 2,\n",
       "       4, 2, 4, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2,\n",
       "       2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4,\n",
       "       4, 2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2,\n",
       "       4, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 2,\n",
       "       4, 2, 2, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and print the Confusion Matrix and the accuracy score of the model and interpret the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82  5]\n",
      " [ 1 49]]\n",
      "SVC model accuracy: 95.62%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"SVC model accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that this is a good model with an accuracy of 95.62%. There are 6 values that were not predicted correctly.\n",
    "\n",
    "Using K-fold cross-validation method (cv = 10) will ensure that the accuracy score calculated only on one test set was not just a lucky occurence. To be more exactly in k-fold, with K=10, the dataset is devided into 10 subsets. At the first iteration the model is trained on the first 9 subsets and tested on the 10th subset. In the second iteration, the model is trained on the first 8 subsets and 10th, and is tested on the 9th subset. In this way there are 10 iterations and we obtain 10 accuracies of the model. Will print the mean accuracy score and the standard deviation  of the score computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.53 %\n",
      "Standard Deviation: 1.91 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92727273, 0.96363636, 0.96363636, 0.98181818, 0.96363636,\n",
       "       0.96363636, 0.98148148, 0.96296296, 1.        , 0.94444444])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "svm_accuracies = cross_val_score(estimator = clf, X = x_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(svm_accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(svm_accuracies.std()*100))\n",
    "svm_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the accuracy score calculated for a only one test set is smaller with 0.92% than the accuracy mean calculatd for 10 test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.4. Choose the best performing model based on the results from performing the k-fold cross-validation. Discuss your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for Logistic Regression model: 96.70 %, Std: 1.97.\n",
      "Mean accuracy for Decision Tree Classifier model: 93.59%, Std: 2.21.\n",
      "Mean accuracy for Random Forest Classifier with 10 nb_trees: 96.15 %, Std: 2.39.\n",
      "Mean accuracy for K-Nearest Neighbors: 96.34 %, Std: 2.44.\n",
      "Mean accuracy for Naive Bayes: 96.16 %, Std: 1.91.\n",
      "Mean accuracy for Support Vector Machine: 96.53 %, Std: 1.91.\n",
      "[94.73053901089192, 91.38701005825551, 93.76084066076734, 93.90304295607386, 94.2457764262916, 94.61833923684192]\n",
      "The best accuracy is: 94.73\n"
     ]
    }
   ],
   "source": [
    "lg_accuracy = logisticRegr_accuracies.mean()*100\n",
    "lg_std = logisticRegr_accuracies.std()*100\n",
    "dtc_accuracy = dtc_accuracies.mean()*100\n",
    "dtc_std = dtc_accuracies.std()*100\n",
    "rfc_accuracy = rfc_accuracies.mean()*100\n",
    "rfc_std = rfc_accuracies.std()*100\n",
    "neigh_accuracy = neigh_accuracies.mean()*100\n",
    "neigh_std = neigh_accuracies.std()*100\n",
    "gnb_accuracy = gnb_accuracies.mean()*100\n",
    "gnb_std = gnb_accuracies.std()*100\n",
    "svm_accuracy = svm_accuracies.mean()*100\n",
    "svm_std = svm_accuracies.std()*100\n",
    "\n",
    "print(\"Mean accuracy for Logistic Regression model: {:.2f} %, Std: {:.2f}.\".format(lg_accuracy, lg_std))\n",
    "print(\"Mean accuracy for Decision Tree Classifier model: {:.2f}%, Std: {:.2f}.\".format(dtc_accuracy, dtc_std))\n",
    "print(\"Mean accuracy for Random Forest Classifier with 10 nb_trees: {:.2f} %, Std: {:.2f}.\".format(rfc_accuracy, rfc_std))\n",
    "print(\"Mean accuracy for K-Nearest Neighbors: {:.2f} %, Std: {:.2f}.\".format(neigh_accuracy, neigh_std))\n",
    "print(\"Mean accuracy for Naive Bayes: {:.2f} %, Std: {:.2f}.\".format(gnb_accuracy, gnb_std))\n",
    "print(\"Mean accuracy for Support Vector Machine: {:.2f} %, Std: {:.2f}.\".format(svm_accuracy, svm_std))\n",
    "\n",
    "lg1 = lg_accuracy-lg_std\n",
    "dtc1 = dtc_accuracy - dtc_std\n",
    "rfc1 = rfc_accuracy - rfc_std\n",
    "neigh1 = neigh_accuracy - neigh_std\n",
    "gnb1 = gnb_accuracy - gnb_std\n",
    "svm1 = svm_accuracy - svm_std\n",
    "\n",
    "list1 = [lg1, dtc1, rfc1, neigh1, gnb1, svm1]\n",
    "print(list1)\n",
    "print(\"The best accuracy is: {:.2f}\".format(max(list1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can observe that the best mean accuracy is for Random Forest Classifier model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 5. For the chosen best performing model select two hyper parameters you would like to tune using the grid search method. Learn what the two chosen hyper parameters do to your model. Look into the description of hyper parameters for your respective model by using the scikit-org API https://scikitlearn.org/stable/modules/classes.html. Discuss here the two hyper parameters you chose and how they impact the model. Choose between 2 and 4 different values for each of these hyper parameters to test with grid search. Explain your choice and, if you have any, make some expectations (you could speculate for example which values of the chosen hyper parameters you believe will make the model perform best and why). Use grid search to find the best model. Print the accuracy of this model and the hyper parameter values of the best model. Do these values confirm your intuition?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's see what parameters has the classifier of  Random Forest Classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"n_estimators\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "          \"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"max_depth\": [1, 2, 3, 4, 5, 6, 7, 8, 9, None],\n",
    "          \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of some hyper parameters that I chose:\n",
    "#### 1. n_estimators - Number of trees in the forest.\n",
    "#### 2. criterion - The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set. Entropy is a measure of information that indicates the disorder of the features with the target.\n",
    "#### 3. max_depth - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "#### 4. max_features - The number of features to consider when looking for the best split:\n",
    "####               - If int, then consider max_features features at each split.\n",
    "####               - If float, then max_features is a fraction and round(max_features * n_features) features are considered at each split.\n",
    "####               - If “auto”, then max_features=sqrt(n_features).\n",
    "####               - If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "####               - If “log2”, then max_features=log2(n_features).\n",
    "####               - If None, then max_features=n_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 97.98 %\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 'sqrt', 'n_estimators': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = rfc,\n",
    "                            param_grid= params,\n",
    "                            cv = 10,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that for the best accuracy we must have the values for parameters as: \n",
    "\"criterion\" = \"gini\",\n",
    "\"max_depth\" = 3, \n",
    "\"max_features\" = sqrt,\n",
    "\"n_estimators\": 6.\n",
    "This means that the number of trees in the forest must be 3. The maximum depth of the tree is 3. The criterion for measure the quality of the split is \"gini\". The nuumber of features to consider when looking for the best split is \"sqrt\" of the total number of features. Using these parameters will obtain an accuracy of 97.98%."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
